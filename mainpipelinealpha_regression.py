# -*- coding: utf-8 -*-
"""MainPipelineAlpha.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12uVchANt3eJ1Nv1NYIzQ77jy_bAh35iD
"""

### Modified May 8th, 2:10PM

use_gdrive = True
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import torch
from torchvision.models.video import r3d_18, R3D_18_Weights
print("we made it here")
# from torchvision.models.video import swin3d_t, Swin3D_T_Weights
print('we did not make it here')
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve
import os
import torch.optim as optim
import random
import json
from mainpipeline_helpers import *


data_dir = "datasets"

### assume we have filelistcsv loaded
FileListCSV = pd.read_csv("FileList.csv")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

train_data_dir = data_dir + "/train_batched_downsampled"
val_data_dir = data_dir + "/val_batched_downsampled"
test_data_dir = data_dir + "/test_batched_downsampled"

#  Initialize r3d model with the best available weights
weights1 = R3D_18_Weights.DEFAULT
model1 = r3d_18(weights=weights1)
model1.eval()

model1_test = R3D_18_Weights.DEFAULT
model1_test = r3d_18(weights=weights1)
model1_test.eval()

#  Another example, swin3d_t model

# model2 = swin3d_t(weights="DEFAULT")
# weights2 = Swin3D_T_Weights.DEFAULT
# model2.eval()

# from torchvision.models.video import swin3d_t, Swin3D_T_Weights
#  Initialize r3d model with the best available weights
#  Another example, swin3d_t model

# model2_test = swin3d_t(weights="DEFAULT")
# weights2 = Swin3D_T_Weights.DEFAULT
# model2_test.eval()

#  Create extended model classes 
# Pretrained r3d and swin3d_t have out_fueature size of (400,1), therefore here we added a linear layer of to reduce size and a sigmoid layer to both models to get desired output form.

model_r3d = r3dmodel(model1, regression=True)
# model_swin3dt = swin3dmodel(model2, regression=True)

model = model_r3d

loss_fn = CustomMSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.00001, weight_decay=0.0005)
model.to(device)
model.train()

batch_size_fake = 1
batch_size_effective = 20

import random
SEED = 1234
os.environ['PYTHONHASHSEED'] = str(SEED)
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
# torch.use_deterministic_algorithms(True)
if torch.cuda.is_available():
    torch.cuda.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)

    # https://pytorch.org/docs/stable/notes/randomness.html
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

num_epochs = 1

### CHANGE EVERY TIME YOU RUN!!!!!!!
experiment_name = 'ID2_May8_2023_regression'

best_model_path = data_dir + '/train_batched_normal_models'

best_val_epoch_R_2 = -1*float('inf')
best_val_pred_ef = None
best_val_actual_ef = None

train_losses = []
val_losses = []

for epoch in range(num_epochs):
  print(f"EPOCH {epoch + 1}")
  print("TRAINING")
  train_mean_loss, train_pred_ef, train_actual_ef, train_vid_ids = epoch_evaluation_regression(model, train_data_dir, loss_fn, optimizer, batch_size_fake, batch_size_effective, device, training=True)

  print("VALIDATION")
  val_mean_loss, val_pred_ef, val_actual_ef, val_vid_ids = epoch_evaluation_regression(model, val_data_dir, loss_fn, optimizer, batch_size_fake, batch_size_effective, device, training=False)

  train_epoch_R_2, train_MAE_score, train_RMSE_score = metrics_regression(train_pred_ef, train_actual_ef)
  val_epoch_R_2, val_MAE_score, val_RMSE_score = metrics_regression(val_pred_ef, val_actual_ef)

  if val_epoch_R_2 > best_val_epoch_R_2:
    best_val_epoch_R_2 = val_epoch_R_2
    best_val_actual_ef = val_actual_ef[:]
    best_val_pred_ef = val_pred_ef[:]
    if f'{experiment_name}.pt' in os.listdir(best_model_path):
      os.remove(f'{best_model_path}/{experiment_name}.pt')
    torch.save(model.state_dict(), f'{best_model_path}/{experiment_name}.pt')
    print(f"Saved new model on epoch {epoch}")

  train_losses.append(train_mean_loss)
  val_losses.append(val_mean_loss)

  print("SUMMARY: ")
  print(f"EPOCH {epoch + 1} -- Train Loss: {train_mean_loss} Val Loss: {val_mean_loss};  Train R^2: {train_epoch_R_2} Val R^2: {val_epoch_R_2};")



### CHANGE EVERY TIME YOU RUN!!!!!!!
best_model_path = data_dir + '/train_batched_normal_models'


### TESTING!!!
test_mean_loss, test_pred_ef, test_actual_ef, test_vid_ids = epoch_evaluation_regression(model, test_data_dir, loss_fn, optimizer, batch_size_fake, batch_size_effective, device, training=False)
### Make sure to change this line!!!
# best_model = swin3dmodel(model2_test)
test_epoch_R_2, test_MAE_score, test_RMSE_score = metrics_regression(test_pred_ef, test_actual_ef)

best_model = r3dmodel(model1_test)

best_model_parameters = torch.load(f'{best_model_path}/{experiment_name}.pt')
best_model.load_state_dict(best_model_parameters)
best_model.to(device)


fig_scatter, ax1 = plt.subplots(nrows=1, ncols=1, figsize=(10, 4))
fig_loss_curve, ax2 = plt.subplots(nrows=1, ncols=1, figsize=(10, 4))

ax1.scatter(test_pred_ef, test_actual_ef, color='black', alpha=0.5)
x_line = np.linspace(0, 100, 100)
y_line = x_line
ax1.plot(x_line, y_line, 'r', label='y=x')

ax1.set_xlabel("Predicted EF")
ax1.set_ylabel("Actual EF")
ax1.legend()

epochs = list(range(1, len(train_losses) + 1))

ax2.plot(epochs, train_losses, label = 'Train Loss')
ax2.plot(epochs, val_losses, label = 'Validation Loss')
ax2.set_xlabel('Epochs')
ax2.set_ylabel('Loss')
ax2.legend()

if experiment_name not in os.listdir():
    os.mkdir(experiment_name)

fig_scatter.savefig(f'{experiment_name}/final_results_{experiment_name}_ax1')
fig_loss_curve.savefig(f'{experiment_name}/final_results_{experiment_name}_ax2')

print(f"TEST SUMMARY: Test R^2: {test_epoch_R_2}, test_MAE_score: {test_MAE_score}, test_RMSE_score: {test_RMSE_score}, test_loss: {test_mean_loss}")

numerical_results = {
  "test_epoch_R_2": test_epoch_R_2,
  "test_MAE_score": test_MAE_score,
  "test_RMSE_score": test_RMSE_score,
  "predicted_ef": test_pred_ef,
  "actual_ef": test_actual_ef,
  "video_ids": test_vid_ids
}

with open(f'{experiment_name}/numerical_results_{experiment_name}.json', 'w') as file:
  json.dump(numerical_results, file, indent=1)
